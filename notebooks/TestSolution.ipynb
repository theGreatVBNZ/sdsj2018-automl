{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "\n",
    "from utils import load, pprint, root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    'check_1_r', 'check_2_r', 'check_3_r', 'check_4_c', 'check_5_c',\n",
    "    'check_6_c', 'check_7_c', 'check_8_c',\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    task, problem = folder.replace('check_', '').split('_')\n",
    "    train_path = f'~/cnt/sdsj2018-automl/data/{folder}/train.csv'\n",
    "    mode = 'regression' if problem == 'r' else 'classification'\n",
    "    output_dir = f'./t{task}'\n",
    "    cmd_train = f'mkdir {output_dir}; python train.py --mode {mode} --train-csv {train_path} --model-dir {output_dir}'\n",
    "    \n",
    "    test_path = f'~/cnt/sdsj2018-automl/data/{folder}/test.csv'\n",
    "    prediction_path = f'{output_dir}/prediction.csv'\n",
    "    cmd_predict = f'python predict.py --test-csv {test_path} --prediction-csv {prediction_path} --model-dir {output_dir}'\n",
    "\n",
    "#     print(f'echo Task {task}')\n",
    "#     print('echo ------------------------------------')\n",
    "#     print(cmd_train)\n",
    "#     print(cmd_predict)\n",
    "#     print('echo ----------***************-----------')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(solution, task):\n",
    "    solution_path = f'~/cnt/sdsj2018-automl/solution/{solution}/t{task}/prediction.csv'\n",
    "    prediction = pd.read_csv(solution_path, index_col=0)\n",
    "    _, target = load(task, 'test-target')\n",
    "    if task < 4:\n",
    "        metrics = np.sqrt(mean_squared_error(prediction['prediction'], target['target']))\n",
    "    else:\n",
    "        metrics = roc_auc_score(target['target'], prediction['prediction'])\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1: RMSE=11.449004887101944\n",
      "Task 2: RMSE=1.6338020824320976\n",
      "Task 3: RMSE=118263.65765372704\n",
      "Task 4: ROC_AUC=0.8618950450488363\n",
      "Task 5: ROC_AUC=0.7717664972379902\n",
      "Task 6: ROC_AUC=0.6551205066635908\n",
      "Task 7: ROC_AUC=0.7269517697683701\n",
      "Task 8: ROC_AUC=0.8627469794780536\n"
     ]
    }
   ],
   "source": [
    "for task in range(1, 9):\n",
    "    metric = 'RMSE' if task < 4 else 'ROC_AUC'\n",
    "    value = score('baseline', task)\n",
    "    print(f'Task {task}: {metric}={value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees\n",
    "Суммарное время тренировки + скоринга ~30 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1: RMSE=9.738509028456058 \n",
      "Difference: -1.7104958586458867\n",
      "\n",
      "Task 2: RMSE=1.6374022353879505 \n",
      "Difference: 0.00360015295585292\n",
      "\n",
      "Task 3: RMSE=118265.1477466668 \n",
      "Difference: 1.4900929397554137\n",
      "\n",
      "Task 4: ROC_AUC=0.7698038617332088 \n",
      "Difference: -0.09209118331562749\n",
      "\n",
      "Task 5: ROC_AUC=0.7737069437580545 \n",
      "Difference: 0.0019404465200643095\n",
      "\n",
      "Task 6: ROC_AUC=0.653936195438504 \n",
      "Difference: -0.001184311225086776\n",
      "\n",
      "Task 7: ROC_AUC=0.8340454679904876 \n",
      "Difference: 0.10709369822211756\n",
      "\n",
      "Task 8: ROC_AUC=0.8808520114337872 \n",
      "Difference: 0.018105031955733653\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task in range(1, 9):\n",
    "    metric = 'RMSE' if task < 4 else 'ROC_AUC'\n",
    "    value = score('trees', task)\n",
    "    baseline_value = score('baseline', task)\n",
    "    diff = value - baseline_value\n",
    "    print(f'Task {task}: {metric}={value} \\nDifference: {diff}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
