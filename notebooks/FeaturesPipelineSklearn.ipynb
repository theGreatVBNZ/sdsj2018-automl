{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка фичей с помощью инструментов sklearn: Pipeline, FeatureUnion, ColumnTransformer, ...<br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer<br>\n",
    "http://scikit-learn.org/stable/modules/compose.html<br>\n",
    "http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html<br>\n",
    "http://michelleful.github.io/code-blog/2015/06/20/pipelines/<br>\n",
    "\n",
    "Фиксируем numeric и categorical columns. Ищем datetime переменные.\n",
    "\n",
    "Для datetime (каждый пункт строится независимо):\n",
    "    - выделение дней, месяцев, ...\n",
    "\n",
    "Для numeric (каждый пункт строится независимо):\n",
    "    - lag-фичи по каждой datetime переменной\n",
    "    - agg-фичи по месяцам, неделям и дням каждой datetime переменной\n",
    "\n",
    "Для categorical (каждый пункт строится независимо):\n",
    "    - dummies\n",
    "   \n",
    "Заполняем пропуски: http://scikit-learn.org/stable/modules/impute.html#impute\n",
    "    \n",
    "Отдельный пайплайн на обработку пропусков в категориальных и числовых признаках:<br>\n",
    "http://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmitrymikhailovich_/cnt/sdsj2018-automl/.direnv/python-3.6.3/lib/python3.6/site-packages/lightgbm/__init__.py:45: FutureWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS will be built by the Apple Clang compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you won't need to install the gcc compiler anymore.\n",
      "Instead of that, you'll need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from functools import partial\n",
    "from IPython.display import display\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load\n",
    "\n",
    "task = 4\n",
    "df_x, target = load(task, 'train')\n",
    "_, y_test = load(task, 'test-target')\n",
    "x_test, _ = load(task, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114130, 142)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиваем колонки по типам значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_columns_by_type(df_x, max_categorical_levels):\n",
    "    max_categorical_levels = 10\n",
    "    datetime_columns, numerical_columns, categorical_columns, idx_columns, single_value_columns = [], [], [], [], []\n",
    "    for col in df_x.columns:\n",
    "        column_unique_values = df_x[col].nunique()\n",
    "        if column_unique_values <= max_categorical_levels:\n",
    "            if column_unique_values == 1:\n",
    "                single_value_columns.append(col)\n",
    "            else:\n",
    "                categorical_columns.append(col)\n",
    "        elif col.startswith('datetime'):\n",
    "            datetime_columns.append(col)\n",
    "        elif col.startswith('number'):\n",
    "            numerical_columns.append(col)\n",
    "        elif col.startswith('id_') or col.endswith('_id'):\n",
    "            idx_columns.append(col)\n",
    "    return datetime_columns, numerical_columns, categorical_columns, idx_columns, single_value_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_columns, numerical_columns, categorical_columns, idx_columns, single_value_columns = group_columns_by_type(df_x, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделение datetime фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer\n",
    "# http://scikit-learn.org/stable/modules/compose.html\n",
    "# http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "# http://michelleful.github.io/code-blog/2015/06/20/pipelines/\n",
    "\n",
    "import datetime\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DatetimeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):            \n",
    "        self.created_features = None\n",
    "        \n",
    "    def transform(self, col):\n",
    "        df_datetime = pd.DataFrame()\n",
    "        col = col.apply(lambda x: self.parse_dt(x))\n",
    "        df_datetime[f'weekday_dt'] = col.apply(lambda x: x.weekday())\n",
    "        df_datetime[f'month_dt'] = col.apply(lambda x: x.month)\n",
    "        df_datetime[f'day_dt'] = col.apply(lambda x: x.day)\n",
    "        df_datetime[f'hour_dt'] = col.apply(lambda x: x.hour)\n",
    "        df_datetime[f'hour_of_week_dt'] = col.apply(lambda x: x.hour + x.weekday() * 24)\n",
    "        df_datetime[f'minute_of_day_dt'] = col.apply(lambda x: x.minute + x.hour * 60)\n",
    "        \n",
    "        if self.created_features is None:\n",
    "            self.created_features = list(df_datetime.columns)\n",
    "        else:\n",
    "            assert self.created_features == list(df_datetime.columns)\n",
    "        return df_datetime\n",
    "    \n",
    "    def fit(self, x, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def parse_dt(self, x):        \n",
    "        if isinstance(x, datetime.datetime) or isinstance(x, datetime.date):\n",
    "            return x\n",
    "        elif not isinstance(x, str):\n",
    "            return np.nan\n",
    "        elif len(x) == len('2010-01-01'):\n",
    "            return datetime.datetime.strptime(x, '%Y-%m-%d')\n",
    "        elif len(x) == len('2010-01-01 10:10:10'):\n",
    "            return datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "        else:\n",
    "            return np.nan\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "        return self.created_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "datetime_transformer = ColumnTransformer([\n",
    "    (col, DatetimeTransformer(), col) \n",
    "    for col in datetime_columns\n",
    "])\n",
    "\n",
    "# _x = datetime_transformer.fit_transform(df_x)\n",
    "# _features = datetime_transformer.get_feature_names()\n",
    "# pd.DataFrame(data=_x, columns=_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LagValues(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, by):\n",
    "        self.by = by\n",
    "        self.created_features = None\n",
    "        \n",
    "    def transform(self, df_x):\n",
    "        columns_to_shift = [col for col in df_x.columns if col != self.by]\n",
    "        df_lags = df_x.sort_values(self.by, ascending=True)\n",
    "        df_lags.drop(self.by, axis=1, inplace=True)\n",
    "        df_lags = df_lags.shift(-1)\n",
    "        df_lags.columns = [f'{col}_shift_1' for col in columns_to_shift]\n",
    "        \n",
    "        if self.created_features is None:\n",
    "            self.created_features = list(df_lags.columns)\n",
    "        else:\n",
    "            assert self.created_features == list(df_lags.columns)\n",
    "        return df_lags\n",
    "        \n",
    "    def fit(self, x, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.created_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_transformer = ColumnTransformer([\n",
    "    (col, LagValues(by=col), [col] + numerical_columns) \n",
    "    for col in datetime_columns\n",
    "])\n",
    "\n",
    "# _x = lag_transformer.fit_transform(df_x)\n",
    "# _features = lag_transformer.get_feature_names()\n",
    "# pd.DataFrame(data=_x, columns=_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnsSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.created_features = None\n",
    "    \n",
    "    def transform(self, df_x):                \n",
    "        df_selected = df_x[self.columns].copy()\n",
    "        self.created_features = list(df_selected)\n",
    "        return df_selected\n",
    "    \n",
    "    def fit(self, x, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.created_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "feature_generator = FeatureUnion([\n",
    "    ('numeric', ColumnsSelector(columns=numerical_columns)),\n",
    "    ('datetime', datetime_transformer),\n",
    "    ('lag_numeric', lag_transformer)\n",
    "], n_jobs=1)\n",
    "\n",
    "# feature_generator.fit_transform(df_x.head(1000))\n",
    "# feature_generator.transform(df_x.tail(1000))\n",
    "# feature_generator.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('feature_generator', feature_generator),\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('model', RandomForestClassifier(n_jobs=1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmitrymikhailovich_/cnt/sdsj2018-automl/.direnv/python-3.6.3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, target, test_size=.33, random_state=123)\n",
    "# pipeline.fit(x_train, y_train)\n",
    "model = RandomForestClassifier().fit(pipeline.fit_transform(x_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8535785535531516"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "pred = model.predict_proba(pipeline.transform(x_test))[:, 1]\n",
    "roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = dict(\n",
    "    n_estimators=np.random.randint(low=10, high=300, size=3),\n",
    "    max_depth=np.random.randint(low=2, high=100, size=3)\n",
    ")\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(verbose=1),\n",
    "    param_distributions=param_dist,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    return_train_score=3,\n",
    "    n_iter=3,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 1s, sys: 2.03 s, total: 4min 3s\n",
      "Wall time: 8min 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 169 out of 169 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=1,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=3, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': array([ 94, 169,  55]), 'max_depth': array([10, 42, 53])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rs.fit(pipeline.fit_transform(x_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>219.792</td>\n",
       "      <td>71.991</td>\n",
       "      <td>52.5877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.57847</td>\n",
       "      <td>5.30964</td>\n",
       "      <td>0.0838532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>2.66334</td>\n",
       "      <td>1.14436</td>\n",
       "      <td>0.49686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.0830659</td>\n",
       "      <td>0.423727</td>\n",
       "      <td>0.0388707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_n_estimators</th>\n",
       "      <td>169</td>\n",
       "      <td>55</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_max_depth</th>\n",
       "      <td>53</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'n_estimators': 169, 'max_depth': 53}</td>\n",
       "      <td>{'n_estimators': 55, 'max_depth': 42}</td>\n",
       "      <td>{'n_estimators': 94, 'max_depth': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>0.881061</td>\n",
       "      <td>0.874039</td>\n",
       "      <td>0.852572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>0.880468</td>\n",
       "      <td>0.86427</td>\n",
       "      <td>0.847883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_score</th>\n",
       "      <td>0.870197</td>\n",
       "      <td>0.865329</td>\n",
       "      <td>0.848393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>0.877242</td>\n",
       "      <td>0.867879</td>\n",
       "      <td>0.849616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.00498748</td>\n",
       "      <td>0.00437687</td>\n",
       "      <td>0.00210042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_train_score</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.908492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_train_score</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_train_score</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.908198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_score</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.907404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_train_score</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00133593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         0  \\\n",
       "mean_fit_time                                      219.792   \n",
       "std_fit_time                                       0.57847   \n",
       "mean_score_time                                    2.66334   \n",
       "std_score_time                                   0.0830659   \n",
       "param_n_estimators                                     169   \n",
       "param_max_depth                                         53   \n",
       "params              {'n_estimators': 169, 'max_depth': 53}   \n",
       "split0_test_score                                 0.881061   \n",
       "split1_test_score                                 0.880468   \n",
       "split2_test_score                                 0.870197   \n",
       "mean_test_score                                   0.877242   \n",
       "std_test_score                                  0.00498748   \n",
       "rank_test_score                                          1   \n",
       "split0_train_score                                       1   \n",
       "split1_train_score                                       1   \n",
       "split2_train_score                                       1   \n",
       "mean_train_score                                         1   \n",
       "std_train_score                                          0   \n",
       "\n",
       "                                                        1  \\\n",
       "mean_fit_time                                      71.991   \n",
       "std_fit_time                                      5.30964   \n",
       "mean_score_time                                   1.14436   \n",
       "std_score_time                                   0.423727   \n",
       "param_n_estimators                                     55   \n",
       "param_max_depth                                        42   \n",
       "params              {'n_estimators': 55, 'max_depth': 42}   \n",
       "split0_test_score                                0.874039   \n",
       "split1_test_score                                 0.86427   \n",
       "split2_test_score                                0.865329   \n",
       "mean_test_score                                  0.867879   \n",
       "std_test_score                                 0.00437687   \n",
       "rank_test_score                                         2   \n",
       "split0_train_score                                      1   \n",
       "split1_train_score                                      1   \n",
       "split2_train_score                                      1   \n",
       "mean_train_score                                        1   \n",
       "std_train_score                                         0   \n",
       "\n",
       "                                                        2  \n",
       "mean_fit_time                                     52.5877  \n",
       "std_fit_time                                    0.0838532  \n",
       "mean_score_time                                   0.49686  \n",
       "std_score_time                                  0.0388707  \n",
       "param_n_estimators                                     94  \n",
       "param_max_depth                                        10  \n",
       "params              {'n_estimators': 94, 'max_depth': 10}  \n",
       "split0_test_score                                0.852572  \n",
       "split1_test_score                                0.847883  \n",
       "split2_test_score                                0.848393  \n",
       "mean_test_score                                  0.849616  \n",
       "std_test_score                                 0.00210042  \n",
       "rank_test_score                                         3  \n",
       "split0_train_score                               0.908492  \n",
       "split1_train_score                               0.905522  \n",
       "split2_train_score                               0.908198  \n",
       "mean_train_score                                 0.907404  \n",
       "std_train_score                                0.00133593  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rs.cv_results_).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=53, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=169, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=1,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'get_asdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-566c2ecd0598>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_asdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'get_asdf'"
     ]
    }
   ],
   "source": [
    "rs.get_asdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
